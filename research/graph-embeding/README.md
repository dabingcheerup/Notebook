# Graph Embeding

即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。Embedding其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。

Word2Vec模型实际上分为了两个部分，

## 第一部分为建立模型，

即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是**这个模型通过训练数据所学得的参数**，例如隐层的权重矩阵——后面我们将会看到这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。

基于训练数据建模的过程，我们给它一个名字叫“Fake Task”

### 构建神经网络

1. 句子中间选取一个input word
2. 定义参数

   skip\_window: 它代表着我们从当前input word的一侧（左边或右边）选取词的数量。如果我们设skip\_window=2，那么我们最终获得窗口中的词（包括input word在内）就是\['The', 'dog'，'barked', 'at'\]。skip\_window=2代表着选取左input word左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小span=2x2=4。

   num\_skip: 它代表着我们从整个窗口中选取多少个不同的词作为我们的output word，当skip\_window=2，num\_skips=2时，我们将会得到两组 \(input word, output word\) 形式的训练数据，即 \('dog', 'barked'\)，\('dog', 'the'\)。

3. 神经网络基于这些训练数据将会输出一个概率分布，这个概率代表着我们的词典中的每个词是output word的可能性。模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现。即我们的模型将会从每对单词出现的次数中习得统计结果。

```text
模型细节
因为神经网络只能接受数值输入，所以要想办法用数值表示这些单词

最常用的办法就是：
    1. 基于训练文档来构建我们自己的词汇表（vocabulary）
    2. 再对单词进行one-hot编码。

模型的输入如果为一个10000维的向量，那么输出也是一个10000维度（词汇表的大小）的向量，它包含了10000个概率，每一个概率代表着当前词是输入样本中output word的概率大小。
```

第二部分是通过模型获取嵌入词向量。

